{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoderLectureNotes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsBEcfd6qteb6jYbBAT1hi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jposyluzny/ENEL645/blob/main/Lectures/Week%205/AutoEncoderLectureNotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ALPmYnXMAl"
      },
      "source": [
        "# Learning Goals:\r\n",
        "\r\n",
        "##### Understand the motivation behind auto-encoders\r\n",
        "##### Learn different ways to design an auto-encoder model\r\n",
        "##### Learn potential applications of auto-encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3j_xD9BXmzI"
      },
      "source": [
        "# Auto Encoders\r\n",
        "\r\n",
        "Auto encoders are unsupervised techniques, meaning we have no labelled data <br>\r\n",
        "Used to learn a representation of your data<br>\r\n",
        "Often the learned representation is in a lower-dimensional space than your input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETSaRgjWYRwO"
      },
      "source": [
        "The left side of the model, from the input to the code, is the encoder side.<br>\r\n",
        "The right side of the model, from the code to the output, is the decoder side. <br>\r\n",
        "Where these two segments intersect (at the code) is called the **bottleneck**, or **latent representation** <br>\r\n",
        "\r\n",
        "Let f represent the Encoder, g represent the Decoder, X represent the input, Z represent the code, and X' represent the output: <br>\r\n",
        "Z = f(X) <br>\r\n",
        "X' = g(Z) <br> \r\n",
        "\r\n",
        "In words, the code is the result of the Encoder performing functions on the input, and the output is the result of the Decoder performing functions on the code. <br> \r\n",
        "The objective here is that we want X roughly equal to X'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz6siGusi4HI"
      },
      "source": [
        "For an image, we would pass the input image into some convolutional layers until it can be flattened, add a fully connected layer to turn it into a latent representation, signifying the end of the encoding portion. <br>\r\n",
        "For decoding, we start with a fully connected layer, pass that to some convolutional layers for deconvolution, and then end up with a reconstructed image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuMdrAaezc90"
      },
      "source": [
        "# Up Sampling\r\n",
        "Opposite effect of max pooling<br>\r\n",
        "Many ways to do it, simplest way is \"nearest neighbor interpolation\"<br>\r\n",
        "UpSampling2D -> Keras layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqs8p7t8z_MX"
      },
      "source": [
        "# Auto-encoder Applications\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Learning representations\r\n",
        "*   Data compression\r\n",
        "*   Denoising\r\n",
        "*   Learning manifolds\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5JPdoA-0JGW"
      },
      "source": [
        "# Summary\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Auto-encoders are unsupervised methods that can be used to learn data representation\r\n",
        "*   They can be either fully connected models or convolutional models\r\n",
        "*   They have large applicability for compression, denoising, and other things\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPv2rdWG0aTl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}